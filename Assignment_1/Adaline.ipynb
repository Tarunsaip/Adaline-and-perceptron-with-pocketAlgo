{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTjjmiLBsAdz"
      },
      "source": [
        "Question #1 In this question you are to create some simulated data sets and then use the Adaline neuron and the Sigmoid to perform some prediction. Use whatever programming language you want to use. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lAxNFuJsEA-"
      },
      "source": [
        "Generate 5000 synthetic data points (x, y) as follows:\n",
        "\n",
        "a.\tUsing the rnorm() function in R (or equivalent in Matlab or Python or etc), create a vector, x, containing 5000 observations drawn from a Gaussian distribution N(0, 1) [ie, a normal distribution with mean 0 and variance 1]. This vector x represents your set of inputs x.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bJy5o-Ar4Sm",
        "outputId": "a19e3913-38ed-4f03-8a9e-f355072f3df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.36160249  0.13075636 -0.04728183]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "X = []\n",
        "for i in range (5000) :\n",
        "  x = np.random.normal(0,1,1)\n",
        "  X.append(x)\n",
        "  X.append(x*x)\n",
        "  X.append(x*x*x)\n",
        "IN = np.array(X)\n",
        "IN = IN.reshape(5000,3)\n",
        "print(IN[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANM20lnAvYyn"
      },
      "source": [
        "b.\tUsing the rnorm() function in R (or equivalent in Matlab or Python or etc), create a vector, eps, containing 5000 observation drawn from a N(0, 0.25) distribution; ie, a normal distribution with mean 0 and variance 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_Kz4SWhr4RF"
      },
      "outputs": [],
      "source": [
        "eps = np.random.normal(0,0.25,(5000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_-uFXtnzSHb"
      },
      "source": [
        "c.\tUsing vectors x and eps, generate a vector y according to the model \n",
        "\n",
        "y = -1 + 0.5x â€“ 2x2 + 0.3x3 + eps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Op77kVpr4Pj",
        "outputId": "89335280-d883-4ec5-b799-13366daa6c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.43314484],\n",
              "       [-0.92787047],\n",
              "       [-1.43642089],\n",
              "       ...,\n",
              "       [-2.93353463],\n",
              "       [-0.92308717],\n",
              "       [-1.32472416]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "y = []\n",
        "count = 0\n",
        "for i in range(0,15000,3) :\n",
        "  y.append (-1+0.5*X[i]-2*(X[i]*X[i])+0.3*(X[i]*X[i]*X[i]) + eps[count]) \n",
        "  count += 1\n",
        "y = np.array(y)\n",
        "print(y.shape)\n",
        "y.reshape((5000,1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RepeatedKFold \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
        "x_train,x_test,y_train,y_test=train_test_split(IN,y,test_size=0.2)"
      ],
      "metadata": {
        "id": "qDsTTU-qAKmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGwjkrSk7XJ4"
      },
      "source": [
        "d.\tImplement the Adaline neuron learning algorithm using (i) batch gradient descent and (ii) stochastic gradient descent, and test and compare them on linear regression over your synthetic data-points. You need not to perform a cross-validation scheme here; only use the whole data set as your training set. Though if you wish, you may perform cross-validation (LOOCV or 10-fold-cv) but without a test set. The initializations, the learning rate, and the stopping criterion are left for you to explore. Think about the reasons why you use a particular strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK4coMZ9r4G9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class adaline(object):\n",
        "    \n",
        "    ## Constructor ##\n",
        "    def __init__(self, X, y):\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.n = X.shape[0]\n",
        "        self.p = X.shape[1]\n",
        "    \n",
        "    ## Methods ##\n",
        "    def empirical_risk(self, X, y, w):\n",
        "\n",
        "        residuals = np.square(y - np.matmul(X, w))\n",
        "        empirical_risk = 0.5*np.sum(residuals)\n",
        "        return empirical_risk\n",
        "    \n",
        "    def l2_deriv(self, y, y_hat, X):\n",
        "\n",
        "        a = -(y.reshape((5000,1)) - y_hat.reshape((5000,1)))\n",
        "        all_grad = self.X * a\n",
        "        averaged_grad = np.mean(all_grad, axis=0)\n",
        "        return averaged_grad\n",
        "    \n",
        "    def fit(self, max_iter, optim = \"GD\", mini_batch_size = None, lr = 0.01):\n",
        "        ## Concatenate intercept into feature matrix\n",
        "        self.X = np.column_stack((np.ones(shape=(self.n,1)), self.X))\n",
        "        ## Init random weights\n",
        "        self.w = np.random.normal(size=self.p+1)\n",
        "        ## Init loss history\n",
        "        self.loss_history = np.zeros(max_iter)\n",
        "        ## Train the adaline \n",
        "        for i in range(max_iter):\n",
        "            y_hat = np.matmul(self.X, self.w)\n",
        "            #y_hat = y_hat.reshape((self.n,1))\n",
        "            self.loss_history[i] = self.empirical_risk(X=self.X, y=self.y, w=self.w)\n",
        "            if optim == \"GD\":\n",
        "                idx = np.arange(self.n)\n",
        "            elif optim == \"SGD\":\n",
        "                if mini_batch_size == None:\n",
        "                    mini_batch_size = int (np.ceil(self.n/10))\n",
        "                idx = np.random.choice(a=np.arange(self.n), size=mini_batch_size, replace=True)\n",
        "            grad = self.l2_deriv(y=self.y[idx], y_hat=y_hat[idx], X=self.X[idx,:])\n",
        "            self.w = self.w - lr*grad\n",
        "            \n",
        "    def dot(K, L):\n",
        "      if len(K) != len(L):\n",
        "          return 0\n",
        "      return sum(i[0] * i[1] for i in zip(K, L))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_adaline = adaline(X=IN, y=y)\n",
        "my_adaline.fit(max_iter=50, lr=0.12)\n",
        "my_adaline.w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZTXxe0OrryE",
        "outputId": "dc4de360-3307-4ed8-c2e7-6fec6254f7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.92794336,  0.47602715, -2.02948546,  0.30450443])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAf8jVqnr4FU",
        "outputId": "44d5510e-145d-4679-fd84-937a1a777f88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.00235568,  0.50951227, -1.99884621,  0.29795022])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "my_adaline = adaline(X=IN, y=y)\n",
        "my_adaline.train(max_iter=100, lr=0.125, optim = \"GD\")\n",
        "my_adaline.w"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Adaline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}